{"cells":[{"cell_type":"markdown","source":["# Fine-tuning SAM 2 using Sentinel 2 data\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"seP8pzbann1x"}},{"cell_type":"markdown","source":["## What is Segment Anything Model\n","[Segment Anything Model](https://segment-anything.com/) is a Ai model developed by [Meta Ai](https://ai.meta.com/) that performs promptable image segmentation. SAM was trained general notion of object definiction, this enables zero-shot generaliztion of new, previously unknown images and objects without additional training. This allows SAM to work on almost every segmentation case. First version of SAM was realased on 5 April 2023 it was trained on  [SA-1B](https://ai.meta.com/datasets/segment-anything/) dataset that was designed for training general-purpose object segmentation models. On July 29th 2024 Meta released [SAM2](https://ai.meta.com/sam2/) second version of their segmentation model builded on the foundations of SAM. SAM2 was designed to solve promptable image and videos segmentation. SAM 2 segmentation is also faster and more accurate then SAM. SAM architecture was extended with streaming memory and model-in-the-loop data engine for real-time video processing. SAM2 was trained additionally on [SA-V](https://ai.meta.com/datasets/segment-anything-video/) dataset that consists of 51K diverse videos.\n","\n","### Purpose of Fine-Tuning SAM 2\n","While SAM 2 model was designed to handle segmentation of even previously unknown objects it may not be enught accurate in not-mainstrem data. In this notebook SAM 2 models will be fine-tuned to improve segmentaion of agricultural fields.\n","\n","Main benefits of Fine-Tuning SAM2 are:\n","*   Imporoved accuraccy\n","*   More ralistic objects maks\n","*   Faster than training from scratch"],"metadata":{"id":"ZqERYNTngQXX"}},{"cell_type":"markdown","source":["To run this notebook make sure that you're using GPU with egought memory. On Google colab L4 GPU should be enought but it's recomended to use A100 GPU.\n","\n","Install required libraries:\n","*   awscli: Amazon Web Services comand line interface, provides interface to interact with AWS.\n","*   sam2: Segmanet Anything Model 2, Meta's model for image segmentation puproses.\n","*   transformers: Interface for working with machine learning models.\n","*   datasets: Simplifies machine lerning dataset managment.\n","\n"],"metadata":{"id":"AESl2Caunm41"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SZfeN2FWQFa"},"outputs":[],"source":["%pip install awscli &> /dev/null\n","%pip install sam2 &> /dev/null\n","%pip install transformers &> /dev/null\n","%pip install datasets &> /dev/null"]},{"cell_type":"code","source":["import os\n","import glob\n","import random\n","import subprocess\n","import gc\n","import requests\n","import awscli\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tifffile  # Handling TIFF images\n","from osgeo import gdal\n","import scipy.ndimage as ndimage\n","import torch\n","from google.colab import drive\n","from PIL import Image\n","import imghdr\n","from datasets import Dataset\n","from sam2.build_sam import build_sam2\n","from sam2.sam2_image_predictor import SAM2ImagePredictor\n"],"metadata":{"id":"VntRqq06O7ah"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Connect to drive and create directory, final fine-tuned models will be saved there for further use."],"metadata":{"id":"HemMBWFriM1w"}},{"cell_type":"code","source":["drive.mount('/content/gdrive', force_remount=True)\n","\n","!mkdir -p /content/gdrive/MyDrive/SAM_trained_models"],"metadata":{"id":"wWVBU4l6pIOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Download training data\n","[Fields of The World (FTW)](https://fieldsofthe.world/) is a dataset designed to enchant machine learning models used in field of remote sensing and GIS. FTW agregates and harmonizes many smaller open datasets into one large dataset with data from 24 countres from across the world. FTW contains approximately 1.6 million parcel boundaries and over 70,000 samples of agricultural fields. Each instance contains [Sentinel 2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2) image including RGB and NIR bands, image is parired with semantic segmentation masks representig agricultrural fields boundaries."],"metadata":{"id":"-DtrAARzJCyB"}},{"cell_type":"code","source":["def download_data(country):\n","  \"\"\"\n","  Download data from Fields of The World for a specific country.\n","\n","  Parameters:\n","  - country: The name of the country.\n","\n","  Returns:\n","  - None\n","  \"\"\"\n","  command = f\"aws s3 sync s3://kerner-lab/fields-of-the-world/{country} ./{country} --endpoint-url=https://data.source.coop --no-sign-request\"\n","  subprocess.run(command, shell=True)\n"],"metadata":{"id":"OAxoXFvX3jx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Country from whom Fields of The World data will be downloaded\n","country= \"sweden\""],"metadata":{"id":"wqBpYTDv-a1m"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZeeLzzNcg71"},"outputs":[],"source":["# Create directories to store data\n","os.makedirs(country, exist_ok=True)\n","\n","# Download data to created directories\n","download_data(country)\n"]},{"cell_type":"markdown","source":["Plot mask and RGB bands of Sentinel 2 image seperately. Each image have 128x128 pixels size, it coresponds to 1536x1536 meters."],"metadata":{"id":"Wov79kHoCIPt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5tRqjxx1MZY"},"outputs":[],"source":["# Plot mask and Sentinel2 RGB bands\n","img = tifffile.imread(\"sweden/s2_images/window_a/g13-5_00003_17.tif\")\n","mask = tifffile.imread(\"sweden/label_masks/semantic_2class/g13-5_00003_17.tif\")\n","\n","fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n","\n","axes[0][0].imshow(np.array(img[:,:,0]), cmap='gray')\n","axes[0][0].set_title(\"Band B02\")\n","\n","axes[0][1].imshow(np.array(img[:,:,1]), cmap='gray')\n","axes[0][1].set_title(\"Band B03\")\n","\n","axes[1][0].imshow(np.array(img[:,:,2]), cmap='gray')\n","axes[1][0].set_title(\"Band B04\")\n","\n","axes[1][1].imshow(mask, cmap='gray')\n","axes[1][1].set_title(\"Mask\")\n","\n","plt.show()\n"]},{"cell_type":"markdown","source":["## Data preprocessing\n","FTW data is but it does reqiure some processing. Firstly Sanetinel 2 image RGB compositon is created. Then image is devied by Sentintinel 2 scale factor (10000), normalized and converted to u-int8 format. On masks morfologic open operation is aplied to get rid of small regions on masks (regions on mask are somtimes build of one pixel). For each region on mask random point will be selected, this points will works as a prompt to finetune SAM 2."],"metadata":{"id":"Dmdd6g0XH5sz"}},{"cell_type":"code","source":["def get_points(binary_mask, n):\n","  \"\"\"\n","  Get n random points from a binary mask.\n","\n","  Parameters:\n","  - binary_mask: The binary mask from which points are to be extracted.\n","  - n: The number of points to be extracted.\n","\n","  Returns:\n","  - random_points: A list of n random points.\n","  \"\"\"\n","\n","  # Morphology erosion to make sure that points won't be near region's border\n","  msk = ndimage.binary_erosion(binary_mask, structure=np.ones((3,3)))\n","\n","  # Create labels for each unique region on mask\n","  labeled_mask, num_features = ndimage.label(msk)\n","\n","  random_points = []\n","\n","  for label_id in range(1, num_features + 1):\n","    # Indices of current region\n","    region_indices = np.argwhere(labeled_mask == label_id)\n","\n","    for i in range(n):\n","      if len(region_indices) > 0:\n","        # Random points for current region\n","        yx = np.array(region_indices[np.random.randint(len(region_indices))])\n","\n","        random_points.append([yx[1], yx[0]])\n","\n","  return random_points"],"metadata":{"id":"-he_b_Wh2C0z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_num_masks(mask):\n","  \"\"\"\n","  Get the number of unique regions in a binary mask.\n","\n","  Parameters:\n","  - mask: The binary mask.\n","\n","  Returns:\n","  - num_features: The number of unique regions in the mask.\n","  \"\"\"\n","\n","  labeled_mask, num_features = ndimage.label(mask)\n","\n","  return num_features"],"metadata":{"id":"C0r-8jvj4k44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(country, dataset):\n","  \"\"\"\n","  Load and preprocess data from a specific country into a dataset.\n","\n","  Parameters:\n","  - country: The name of the country.\n","  - dataset: The dataset to which the data will be added.\n","\n","  Returns:\n","  - None\n","  \"\"\"\n","\n","  # Save mask paths to lists\n","  masks_names = glob.glob(f\"{country}/label_masks/semantic_2class/*.tif\")\n","\n","\n","  # Empty list to store readed data\n","  images = []\n","  masks = []\n","  points = []\n","  num_masks = []\n","\n","  print(f\"Loading data from: {country}, number of images: {len(masks_names)}\")\n","\n","  # for pth in zip(masks_names, images_names):\n","  for mask_path in masks_names:\n","    mask_name = os.path.splitext(os.path.basename(mask_path))[0]\n","\n","    image_path = f\"{country}/s2_images/window_a/\" + mask_name + \".tif\"\n","\n","    if imghdr.what(mask_path) != 'tiff' and imghdr.what(image_path) != 'tiff':\n","      print(f\"Warning: Skipping file - not a TIFF file.\")\n","      continue\n","\n","\n","    # Read image and mask\n","    try:\n","      mask = tifffile.imread(mask_path)\n","      image = tifffile.imread(image_path)\n","    except Exception as e:\n","      print(e)\n","      continue\n","\n","    # Morfologic open operation to get rid of small masks\n","    mask = ndimage.binary_opening(mask, structure=np.ones((3,3)))\n","\n","    # Rejecting mask without objects\n","    if mask.max() != 0:\n","      # Create RGB image\n","      image = image[:, :, [2, 1, 0]]\n","\n","      # Scale Senetinel2 pixels values\n","      min_reflectance = 0\n","      max_reflectance = np.percentile(image / 10000, 98)  # Clip at the 98th percentile\n","      image_clip = np.clip(image / 10000, min_reflectance, max_reflectance)\n","      image_norm = (image_clip - min_reflectance) / (max_reflectance - min_reflectance)\n","      image_uint8 = (image_norm * 255).astype(np.uint8)\n","\n","\n","      # Add image to list\n","      images.append(Image.fromarray(image_uint8).convert('RGB'))\n","\n","      # Convert mask to image format\n","      mask = Image.fromarray(mask)\n","\n","      # Add mask to list\n","      masks.append(mask)\n","\n","      # Get points for every region on mask\n","      p = get_points(mask, 2)\n","\n","      points.append(p)\n","\n","      # Add number of regions to list\n","      num_masks.append(len(p))\n","\n","  # Add data from current country to dataset\n","  dataset[\"image\"].extend([i for i in images])\n","  dataset[\"label\"].extend([m for m in masks])\n","  dataset[\"points\"].extend([p for p in points])\n","  dataset[\"num_masks\"].extend([n for n in num_masks])\n"],"metadata":{"id":"JwuVIPeC4MmH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading downloaded data, preprocessing it and converting to dataset for better data managment. Loaded dataset contains 4180 images from Sweden."],"metadata":{"id":"S1tfWlgFgr8R"}},{"cell_type":"code","source":["# Create empty dict to store FTW data\n","dataset = {\n","    \"image\": [],\n","    \"label\": [],\n","    \"points\": [],\n","    \"num_masks\": []\n","}\n","\n","# Load FTW data and store it in dict\n","load_data(country, dataset)\n","\n","# Convert dictionary to dataset\n","dataset = Dataset.from_dict(dataset)\n"],"metadata":{"id":"4Qk6CtXE6-7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["View dataset to make sure that dataset is correctly created."],"metadata":{"id":"y-LASDockoaq"}},{"cell_type":"code","source":["print(dataset)\n"],"metadata":{"id":"QF_F8A_0790L","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data is splited to train (80%) and validate (20%) sub-datasets."],"metadata":{"id":"Rbvr4uplzpCV"}},{"cell_type":"code","source":["# Split the dataset into training (80%) and validation (20%)\n","split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n","\n","# Access the training and validation datasets\n","train_dataset = split_dataset[\"train\"]\n","val_dataset = split_dataset[\"test\"]\n"],"metadata":{"id":"Xx_jaV5tBu7W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["View datasets after split. Training dataset contains 3344 images and test dataset 837."],"metadata":{"id":"ArdN6nY6lHMg"}},{"cell_type":"code","source":["print(train_dataset)\n","print(val_dataset)\n"],"metadata":{"id":"TPazlB2AEAyb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot image and mask both with random selected points for each region. Each region have 2 random points. Points are distanted from regions border by using erosion, this will model to properly segment agricultural fields."],"metadata":{"id":"Uwrp_VRNHLp4"}},{"cell_type":"code","source":["# Plot image and mask with random points\n","data = train_dataset[np.random.randint(len(train_dataset))]\n","\n","fig, axs = plt.subplots(1, 2, figsize=(10, 10))\n","\n","# Add image with points\n","axs[0].imshow(data[\"image\"])\n","axs[0].set_title(\"Sentinel image\")\n","for point in data[\"points\"]:\n","    axs[0].plot(point[0], point[1], 'ro')\n","\n","# Add mask with points\n","axs[1].imshow(data['label'], cmap=\"gray\")\n","axs[1].set_title(\"Mask\")\n","for point in data[\"points\"]:\n","    axs[1].plot(point[0], point[1], 'ro')\n","\n","plt.savefig(\"plot_random_points_image.png\", dpi=300)\n","plt.show()\n"],"metadata":{"id":"jB7ChkIO5385"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define function to read random batch from dataset. Function adjusts mask and points shapes to meet training process requirements format."],"metadata":{"id":"cvrvW1BiIeHA"}},{"cell_type":"code","source":["def read_random_batch(dataset):\n","  \"\"\"\n","  Read a random batch from a dataset.\n","\n","  Parameters:\n","  - dataset: The dataset from which the batch is to be read.\n","\n","  Returns:\n","  - batch: A random batch from the dataset.\n","  \"\"\"\n","\n","  # Read random batch from dataset\n","  rnd = np.random.randint(len(dataset))\n","  batch = dataset[rnd]\n","\n","  # Convert to numpy array\n","  img =  np.array(batch[\"image\"])\n","  binary_mask = np.array(batch[\"label\"])\n","  points = np.array(batch[\"points\"])\n","\n","  # Expand mask dimentions\n","  binary_mask = np.expand_dims(binary_mask, axis=-1)\n","  binary_mask = binary_mask.transpose((2, 0, 1))\n","\n","  # Expand points dimentions\n","  points = np.expand_dims(points, axis=1)\n","\n","  return img, binary_mask, points, batch[\"num_masks\"], rnd\n"],"metadata":{"id":"U0bAJIyVay3S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test function by reading random batch and ploting it."],"metadata":{"id":"HRV1QpJTIMSd"}},{"cell_type":"code","source":["# Plot random image and mask from random batch\n","img, mask, points, num_masks, _= read_random_batch(dataset)\n","\n","# Reduce mask dimentions for plot\n","mask = np.squeeze(mask, axis=0)\n","\n","fig, axs = plt.subplots(1, 2, figsize=(10, 10))\n","\n","# Add image\n","axs[0].imshow(img)\n","axs[0].set_title(\"Sentinel image\")\n","\n","# Add mask\n","axs[1].imshow(mask, cmap=\"gray\")\n","axs[1].set_title(\"Mask\")\n","\n","plt.savefig(\"image_and_mask.png\", dpi=300)\n","fig.show()"],"metadata":{"id":"sDMLtL4k4c3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Download SAM2 models checkpoints\n","To fine-tune SAM2 pre-trained model weights are needed. This weights will work as a starting point for futher improving models accuracy in segmentation of agriculture fields. SAM 2 comes with 4 models checkpoints with different in sizes and speed:\n","\n","| Model | Size (M) | Speed (FPS) |\n","|----------|----------|----------|\n","| sam2_hiera_tiny   | 38.9 | 47.2  |\n","| sam2_hiera_small   | 46  | 43.3 (53.0 compiled)   |\n","| sam2_hiera_base_plus    | 80.8  | 34.8 (43.8 compiled)   |\n","| sam2_hiera_large    | 224.4  | 24.2 (30.2 compiled)  |"],"metadata":{"id":"YkIqH0HWu2B-"}},{"cell_type":"code","source":["!wget -O sam2_hiera_tiny.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt\"\n","!wget -O sam2_hiera_small.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt\"\n","!wget -O sam2_hiera_base_plus.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt\"\n","!wget -O sam2_hiera_large.pt \"https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\"\n"],"metadata":{"id":"V-QgGecK7uiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-Tune model\n","Clear cuda memory every time before changing SAM2 model, this prevents GPU memory over folow."],"metadata":{"id":"RH5aYVXavEMm"}},{"cell_type":"code","source":["gc.collect()\n","torch.cuda.empty_cache()\n"],"metadata":{"id":"8HOe0hBMu-v6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sam2_checkpoint = \"sam2_hiera_small.pt\"  # @param [\"sam2_hiera_tiny.pt\", \"sam2_hiera_small.pt\", \"sam2_hiera_base_plus.pt\", \"sam2_hiera_large.pt\"]\n","model_cfg = \"sam2_hiera_s.yaml\" # @param [\"sam2_hiera_t.yaml\", \"sam2_hiera_s.yaml\", \"sam2_hiera_b+.yaml\", \"sam2_hiera_l.yaml\"]\n","\n","sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n","predictor = SAM2ImagePredictor(sam2_model) # Create net\n"],"metadata":{"id":"P8C9DJej7dtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Enable train of model parts\n","predictor.model.sam_mask_decoder.train(True)\n","predictor.model.image_encoder.train(True)\n","predictor.model.sam_prompt_encoder.train(True)\n","\n","# Configure AdamW optimizer\n","optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=0.0001, weight_decay=1e-4)\n","\n","# Mix precision\n","scaler = torch.amp.GradScaler()\n","\n","# Number of steps per epoch to train the model\n","STEPS_PER_EPOCH = 500 # @param\n","\n","# Fine-tuned model name to be saved\n","FINE_TUNED_MODEL_NAME = \"fine_tuned_sam2_small\" # @param\n","\n","# Number of epochs\n","EPOCHS = 15 # @param\n"],"metadata":{"id":"NlSaBsHF70DG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize scheduler\n","accumulation_steps = 2\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEPS_PER_EPOCH, gamma=0.95)\n","mean_iou = 0\n","drive_model_path = \"\"\n","accuraces = []\n","\n","for epoch in range(EPOCHS):\n","  print(f\"Epoch {epoch}\")\n","  for step in range(STEPS_PER_EPOCH):\n","    with torch.amp.autocast('cuda'):\n","        image, mask, input_point, num_masks, rnd = read_random_batch(train_dataset)\n","        input_label = np.ones((num_masks, 1))\n","\n","        predictor.set_image(image)\n","\n","        mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(\n","            input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n","\n","        if unnorm_coords is None or labels is None or unnorm_coords.shape[0] == 0 or labels.shape[0] == 0:\n","           continue\n","\n","        sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n","            points=(unnorm_coords, labels), boxes=None, masks=None,)\n","\n","        # Mask decoder\n","        batched_mode = unnorm_coords.shape[0] > 1\n","\n","        high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n","\n","        low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n","            image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n","            image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n","            sparse_prompt_embeddings=sparse_embeddings,\n","            dense_prompt_embeddings=dense_embeddings,\n","            multimask_output=True,\n","            repeat_image=batched_mode,\n","            high_res_features=high_res_features,\n","        )\n","        prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n","\n","        # Loss\n","        gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n","        prd_mask = torch.sigmoid(prd_masks[:, 0])\n","        seg_loss = (-gt_mask * torch.log(prd_mask + 0.000001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n","\n","        inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n","        iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n","        score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n","        loss = seg_loss + score_loss * 0.05\n","\n","        # Apply gradient accumulation\n","        loss = loss / accumulation_steps\n","        scaler.scale(loss).backward()\n","\n","        # Clip gradients\n","        torch.nn.utils.clip_grad_norm_(predictor.model.parameters(), max_norm=1.0)\n","\n","        if step % accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            predictor.model.zero_grad()\n","\n","        # Update scheduler\n","        scheduler.step()\n","\n","        mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n","\n","        if step % 100 == 0:\n","            accuraces.append(mean_iou)\n","            print(f\"Step {step}:\\t Accuracy (IoU) = {mean_iou:.3f}, Loss = {loss.item():.3f}, LR = {scheduler.get_last_lr()[0]:.7f}\")\n","\n","        if step % 100 == 0:\n","            pass\n","\n","  gc.collect()\n","  torch.cuda.empty_cache()\n","\n","  torch.save(predictor.model.state_dict(), f\"{FINE_TUNED_MODEL_NAME}_epoch_{epoch}\" + \".torch\")\n","\n","  drive_model_path = f\"/content/gdrive/MyDrive/SAM_trained_models/{FINE_TUNED_MODEL_NAME}_epoch_{epoch}.torch\"\n","\n","torch.save(predictor.model.state_dict(), drive_model_path)\n","\n","torch.cuda.empty_cache()"],"metadata":{"id":"4K-2q4gD75Td"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot training accuracy\n","x = range(len(accuraces))\n","x = np.array(x) / 5\n","\n","plt.plot(x, accuraces, color=\"green\", linestyle=\"-\")\n","\n","plt.title(\"Fine-tuning accuracy of small model\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","\n","plt.show()"],"metadata":{"id":"g3r9WRUKma7h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Verify model accuracy\n","To verify models accuracy the intersection over union (IoU) metric will be used. IoU also known as Jaccard's index is a crucial metric in image segmentation, it measures how well model distinguishes objects from background. IoU is the ratio of intersection of ground truth mask with segmented mask and their union area.\n","\n","Formula for Jaccard's Index:\n","\n","$J[A, B] = \\frac{|A \\cap B|}  {|A \\cup B|}$.\n","\n","\n","\n"],"metadata":{"id":"ax1LbuJGe_8Y"}},{"cell_type":"code","source":["def calculate_iou(image1, image2):\n","  \"\"\"\n","  Calculate intersection over union (IoU) between two binary images.\n","\n","  Parameters:\n","  - image1: The first binary image.\n","  - image2: The second binary image.\n","\n","  Returns:\n","  - iou: The IoU value between the two images.\n","  \"\"\"\n","\n","  intersection = np.sum(np.logical_and(image1, image2))  # Pixels where both are 1\n","  union = np.sum(np.logical_or(image1, image2))  # Pixels where either is 1\n","\n","  # Calculate IoU\n","  iou = intersection / union\n","\n","  return(iou)"],"metadata":{"id":"Md95YvH16lnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def segment_image(predictor, image, input_points, origin_mask, fine_tune):\n","    \"\"\"\n","    Segment an image using a fine-tuned model and visualize the segmentation.\n","\n","    Parameters:\n","    - predictor: The trained model used for segmentation.\n","    - image: The input image (e.g., a Sentinel image).\n","    - input_points: The coordinates of the input points used for the model.\n","    - origin_mask: The original mask to compare the segmented result.\n","\n","    Returns:\n","    - seg_map: The final segmented map.\n","    - fig: The matplotlib figure containing the visualization.\n","    \"\"\"\n","\n","    # Create segmentation masks\n","    with torch.no_grad():\n","        predictor.set_image(image)\n","\n","        masks, scores, logits = predictor.predict(\n","            point_coords=input_points,\n","            point_labels=np.ones([input_points.shape[0], 1])\n","        )\n","\n","    # Process masks\n","    np_masks = np.array(masks[:, 0])\n","\n","    # If dimensions number is 1, expand to 2 dims\n","    if scores.ndim == 1:\n","      scores = np.expand_dims(scores, axis=1)  # Adds a new axis\n","\n","    np_scores = scores[:, 0]\n","\n","    # Sort masks by scores\n","    sorted_masks = np_masks[np.argsort(np_scores)][::-1]\n","\n","    # Initialization of segmentation mask and occupancy mask\n","    seg_map = np.zeros_like(sorted_masks[0], dtype=np.uint8)\n","    occupancy_mask = np.zeros_like(sorted_masks[0], dtype=bool)\n","\n","    # Merge all mask into one segemntation map\n","    for i in range(sorted_masks.shape[0]):\n","        mask = sorted_masks[i]\n","        if (mask * occupancy_mask).sum() / mask.sum() > 0.2:\n","          continue\n","\n","        mask_bool = mask.astype(bool)\n","        mask_bool[occupancy_mask] = False\n","        seg_map[mask_bool] = i + 1\n","        occupancy_mask[mask_bool] = True  # Update occupancy_mask\n","\n","    seg_map = seg_map.astype(bool)\n","\n","    # Calculate iou\n","    iou = calculate_iou(seg_map, origin_mask)\n","\n","    # Visualization\n","    if fine_tune:\n","      segmentstion_title = \"Fine-Tuned Model Segmentation\"\n","    else:\n","      segmentstion_title = \"Pre-Trained Model Segmentation\"\n","\n","    fig, axs = plt.subplots(1, 3, figsize=(10, 10))\n","\n","    # Add Sentinel2 image\n","    axs[0].imshow(image)\n","    axs[0].set_title(\"Sentinel Image\")\n","\n","    # Add grount truth mask\n","    axs[1].imshow(origin_mask, cmap=\"gray\")\n","    axs[1].set_title(\"Original Mask\")\n","\n","    # Add final segmentation map\n","    axs[2].imshow(seg_map, cmap=\"gray\")\n","    axs[2].set_title(segmentstion_title)\n","\n","    # Stops image showing\n","    plt.close(fig)\n","\n","    return seg_map, iou, fig"],"metadata":{"id":"VmFI1nBt6NOy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pre-trained model"],"metadata":{"id":"4eexb-3i3O07"}},{"cell_type":"code","source":["sam2_checkpoint = \"sam2_hiera_large.pt\"  # @param [\"sam2_hiera_tiny.pt\", \"sam2_hiera_small.pt\", \"sam2_hiera_base_plus.pt\", \"sam2_hiera_large.pt\"]\n","model_cfg = \"sam2_hiera_l.yaml\" # @param [\"sam2_hiera_t.yaml\", \"sam2_hiera_s.yaml\", \"sam2_hiera_b+.yaml\", \"sam2_hiera_l.yaml\"]\n","\n","# Build SAM2 net\n","sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n","predictor = SAM2ImagePredictor(sam2_model)"],"metadata":{"id":"_ObQIxs6-O0d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["First pre-trained model will be tested. Results of segmentation are shown on plot where sentinel ground truth mask and final segmentation mask are compare. Usuall pre-trained models"],"metadata":{"id":"NEXKr-W8Tu3k"}},{"cell_type":"code","source":["iou_list = []\n","\n","for i in range(50):\n","  image, mask, input_points, region_num, id = read_random_batch(val_dataset)\n","  mask = np.squeeze(mask, axis=0)\n","\n","  origin_mask = mask\n","\n","  seg_map, iou, fig = segment_image(predictor, image, input_points, origin_mask, fine_tune=False)\n","\n","  iou_list.append(iou)\n","\n","mean_iou = np.mean(iou_list)\n","\n","print(f\"Mean IoU: {mean_iou}\")\n","\n","# Save results to file\n","file_name=\"/content/gdrive/MyDrive/SAM_trained_models/iou_results.txt\"\n","with open(file_name, 'a') as f:\n","  f.write(f\"Pretrained {sam2_checkpoint} Accuracy: {mean_iou}\\n\")\n"],"metadata":{"id":"TTysfNF6BSAS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-Tuned model"],"metadata":{"id":"KFS8awlH3VwO"}},{"cell_type":"code","source":["sam2_checkpoint = \"sam2_hiera_large.pt\"  # @param [\"sam2_hiera_tiny.pt\", \"sam2_hiera_small.pt\", \"sam2_hiera_base_plus.pt\", \"sam2_hiera_large.pt\"]\n","model_cfg = \"sam2_hiera_l.yaml\" # @param [\"sam2_hiera_t.yaml\", \"sam2_hiera_s.yaml\", \"sam2_hiera_b+.yaml\", \"sam2_hiera_l.yaml\"]\n","\n","tiny = \"/content/gdrive/MyDrive/SAM_trained_models/fine_tuned_sam2_tiny_epoch_14.torch\"\n","small = \"/content/gdrive/MyDrive/SAM_trained_models/fine_tuned_sam2_small_epoch_14.torch\"\n","base_plus = \"/content/gdrive/MyDrive/SAM_trained_models/fine_tuned_sam2_base_epoch_14.torch\"\n","large = \"/content/gdrive/MyDrive/SAM_trained_models/fine_tuned_sam2_large_epoch_14.torch\"\n","\n","# Choose fine-tuned model weights\n","FINE_TUNED_MODEL_WEIGHTS = \"large\" # @param [\"tiny\", \"small\", \"base_plus\", \"large\"]\n","FINE_TUNED_MODEL_WEIGHTS = eval(FINE_TUNED_MODEL_WEIGHTS)\n","\n","# Build SAM2 net and load fine-tuned weights\n","sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n","predictor = SAM2ImagePredictor(sam2_model)\n","predictor.model.load_state_dict(torch.load(FINE_TUNED_MODEL_WEIGHTS, weights_only=True))\n"],"metadata":{"id":"0QJL9FR3b3eB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iou_list = []\n","\n","for i in range(50):\n","  image, mask, input_points, region_num, _ = read_random_batch(val_dataset)\n","  mask = np.squeeze(mask, axis=0)\n","\n","  origin_mask = mask\n","\n","  seg_map, iou, fig = segment_image(predictor, image, input_points, origin_mask, fine_tune=True)\n","\n","  iou_list.append(iou)\n","\n","mean_iou = np.mean(iou_list)\n","\n","print(f\"Mean IoU: {mean_iou}\")\n","\n","# # Save results to file\n","file_name=\"/content/gdrive/MyDrive/SAM_trained_models/iou_results.txt\"\n","with open(file_name, 'a') as f:\n","  f.write(f\"Finetune {sam2_checkpoint} Accuracy: {mean_iou}\\n\")"],"metadata":{"id":"EqvHKHmNDnTn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Compare results"],"metadata":{"id":"2a6GqdEr3dMd"}},{"cell_type":"markdown","source":["Fine-tuned model weights are"],"metadata":{"id":"JifZaY-zTg8S"}},{"cell_type":"code","source":["# Read random batch\n","image, mask, input_points, region_num, _ = read_random_batch(val_dataset)\n","mask = np.squeeze(mask, axis=0)\n","\n","sam2_checkpoint = \"sam2_hiera_tiny.pt\"  # @param [\"sam2_hiera_tiny.pt\", \"sam2_hiera_small.pt\", \"sam2_hiera_base_plus.pt\", \"sam2_hiera_large.pt\"]\n","model_cfg = \"sam2_hiera_t.yaml\" # @param [\"sam2_hiera_t.yaml\", \"sam2_hiera_s.yaml\", \"sam2_hiera_b+.yaml\", \"sam2_hiera_l.yaml\"]\n","\n","# Build SAM2 net and load fine-tuned weights\n","sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n","predictor = SAM2ImagePredictor(sam2_model)\n","\n","tiny = \"/content/gdrive/MyDrive/SAM_trained_models/fine_tuned_sam2_tiny_epoch_14.torch\"\n","small = \"/content/gdrive/MyDrive/SAM_trained_models/fine_tuned_sam2_small_epoch_14.torch\"\n","base_plus = \"/content/gdrive/MyDrive/SAM_trained_models/fine_tuned_sam2_base_epoch_14.torch\"\n","large = \"/content/gdrive/MyDrive/SAM_trained_models/fine_tuned_sam2_large_epoch_14.torch\"\n","\n","# Choose fine-tuned model weights\n","FINE_TUNED_MODEL_WEIGHTS = \"tiny\" # @param [\"tiny\", \"small\", \"base_plus\", \"large\"]\n","FINE_TUNED_MODEL_WEIGHTS = eval(FINE_TUNED_MODEL_WEIGHTS)\n"],"metadata":{"id":"OkI2U-3OOGGk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seg_map, iou, fig = segment_image(predictor, image, input_points, mask, fine_tune=False)\n","\n","fig\n"],"metadata":{"id":"NDUP-X0QNXsS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load weights\n","predictor.model.load_state_dict(torch.load(FINE_TUNED_MODEL_WEIGHTS))\n","\n","seg_map2, iou2, fig2 = segment_image(predictor, image, input_points, mask, fine_tune=True)\n","\n","fig2\n"],"metadata":{"id":"sJNG6-s1N__R"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"L4","collapsed_sections":["YkIqH0HWu2B-"],"authorship_tag":"ABX9TyNne0plvV2XxGf1TWm1MMKJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}